{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['-r'], dest='data_ration', nargs=None, const=None, default=None, type=None, choices=None, help='REQUIRED: Downsampling ratio of your downsampled data [example: 16]', metavar=None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DFHiC/generate_train_data.py 수정\n",
    "\n",
    "import os, sys, math, random, argparse\n",
    "import numpy as np\n",
    "\n",
    "# cell = \"GM12878\"\n",
    "# ref_chrom = \"hg19.txt\"\n",
    "# data_ratio = \"16\"\n",
    "\n",
    "# input_data_dir = f'{path}/data'\n",
    "# input_downsample_dir = f'{path}/data_downsampled_{data_ratio}'\n",
    "\n",
    "path = os.getcwd()\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Read Hi-C contact map and Divide submatrix for train and predict', add_help=True)\n",
    "req_args = parser.add_argument_group('Required Arguments')\n",
    "req_args.add_argument('-i', dest='input_data_dir', required=True,\n",
    "                      help='REQUIRED: Input Hi-C data directory, containing .txt files (Hi-C contact pares) [example: /home/data]')\n",
    "req_args.add_argument('-d', dest='input_downsample_dir', required=True,\n",
    "                      help='REQUIRED: Input Hi-C downsampled data directory, containing .txt files (Hi-C contact pares) [example: /home/data_downsampled_16]')\n",
    "req_args.add_argument('-m', dest='model', required=True, choices=['HiCARN', 'DeepHiC', 'HiCNN2', 'HiCSR', 'DFHiC', 'hicplus', 'SRHiC'],\n",
    "                      help='REQUIRED: Model name that you want to use [example: DFHiC]')\n",
    "req_args.add_argument('-g', dest='ref_chrom', required=True,\n",
    "                      help='REQUIRED: Reference genome length file, your data is based on [example: hg19.txt]')\n",
    "req_args.add_argument('-r', dest='data_ration', required=True,\n",
    "                      help='REQUIRED: Downsampling ratio of your downsampled data [example: 16]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크로모좀 별로 matrix 만들기\n",
    "def hic_matrix_extraction(res=10000, norm_method='NONE'):\n",
    "    chrom_list = list(range(1,23))#chr1-chr22\n",
    "    hr_contacts_dict={}\n",
    "    for each in chrom_list:\n",
    "        hr_hic_file = f'{input_data_dir}/chr{each}_10kb.txt'\n",
    "        chrom_len = {item.split()[0]:int(item.strip().split()[1]) for item in open(f'{ref_chrom}').readlines()} # GM12878 Hg19\n",
    "        mat_dim = int(math.ceil(chrom_len[f'chr{each}']*1.0/res))\n",
    "        hr_contact_matrix = np.zeros((mat_dim,mat_dim))\n",
    "        for line in open(hr_hic_file).readlines():\n",
    "            idx1, idx2, value = int(line.strip().split('\\t')[0]),int(line.strip().split('\\t')[1]),float(line.strip().split('\\t')[2])\n",
    "            if idx2/res>=mat_dim or idx1/res>=mat_dim:\n",
    "                continue\n",
    "            else:\n",
    "                hr_contact_matrix[int(idx1/res)][int(idx2/res)] = value\n",
    "        hr_contact_matrix+= hr_contact_matrix.T - np.diag(hr_contact_matrix.diagonal())\n",
    "        hr_contacts_dict[f'chr{each}'] = hr_contact_matrix\n",
    "    lr_contacts_dict={}\n",
    "    for each in chrom_list:\n",
    "        lr_hic_file = f'{input_downsample_dir}/chr{each}_10kb.txt'\n",
    "        chrom_len = {item.split()[0]:int(item.strip().split()[1]) for item in open('chromosome.txt').readlines()}\n",
    "        mat_dim = int(math.ceil(chrom_len[f'chr{each}']*1.0/res))\n",
    "        lr_contact_matrix = np.zeros((mat_dim,mat_dim))\n",
    "        for line in open(lr_hic_file).readlines():\n",
    "            idx1, idx2, value = int(line.strip().split('\\t')[0]),int(line.strip().split('\\t')[1]),float(line.strip().split('\\t')[2])\n",
    "            if idx2/res>=mat_dim or idx1/res>=mat_dim:\n",
    "                continue\n",
    "            else:\n",
    "                lr_contact_matrix[int(idx1/res)][int(idx2/res)] = value\n",
    "        lr_contact_matrix+= lr_contact_matrix.T - np.diag(lr_contact_matrix.diagonal())\n",
    "        lr_contacts_dict[f'chr{each}'] = lr_contact_matrix\n",
    "\n",
    "    nb_hr_contacts={item:sum(sum(hr_contacts_dict[item])) for item in hr_contacts_dict.keys()} # read 수\n",
    "    nb_lr_contacts={item:sum(sum(lr_contacts_dict[item])) for item in lr_contacts_dict.keys()}\n",
    "     \n",
    "    return hr_contacts_dict,lr_contacts_dict,nb_hr_contacts,nb_lr_contacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 매트릭스 자르기\n",
    "# submat_size=40\n",
    "def crop_hic_matrix_by_chrom(chrom, size=submat_size, thred=200, model='model'): # thred=2M/resolution\n",
    "    chr = int(chrom.split('chr')[1])\n",
    "    distance=[]\n",
    "    crop_mats_hr=[]\n",
    "    crop_mats_lr=[]\n",
    "    coordinates_hr=[]    \n",
    "    coordinates_lr=[]    \n",
    "\n",
    "    row,col = hr_contacts_dict[chrom].shape\n",
    "    if row<=thred or col<=thred: # bin 수가 200 보다 작으면 False\n",
    "        print('HiC matrix size wrong!')\n",
    "        sys.exit()\n",
    "    def quality_control(mat,thred=0.05):\n",
    "        if len(mat.nonzero()[0])<thred*mat.shape[0]*mat.shape[1]: # 숫자 있는 셀의 수가 전체의 5% 미만이면 False\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    \n",
    "    if size == 40:\n",
    "        if model == \"HiCNN\":\n",
    "            for idx1 in range(0,row-size,28):\n",
    "                for idx2 in range(0,col-size,28):\n",
    "                    if abs(idx1-idx2)<thred:\n",
    "                        if quality_control(lr_contacts_dict[chrom][idx1:idx1+size,idx2:idx2+size]):\n",
    "                            distance.append([idx1-idx2,chrom])\n",
    "                            coordinates_hr.append([chr, idx1, idx2])\n",
    "                            coordinates_lr.append([chr, idx1, idx2])\n",
    "                            \n",
    "                            hr_contact = hr_contacts_dict[chrom][idx1:idx1+size,idx2:idx2+size]\n",
    "                            lr_contact = lr_contacts_dict[chrom][idx1:idx1+size,idx2:idx2+size]\n",
    "\n",
    "                            crop_mats_hr.append(hr_contact)                \n",
    "                            crop_mats_lr.append(lr_contact)\n",
    "\n",
    "            crop_mats_hr = np.concatenate([item[np.newaxis,:] for item in crop_mats_hr],axis=0)\n",
    "            crop_mats_lr = np.concatenate([item[np.newaxis,:] for item in crop_mats_lr],axis=0)\n",
    "\n",
    "        else:        \n",
    "            for idx1 in range(0,row-size,size):\n",
    "                for idx2 in range(0,col-size,size):\n",
    "                    if abs(idx1-idx2)<thred:\n",
    "                        if quality_control(lr_contacts_dict[chrom][idx1:idx1+size,idx2:idx2+size]):\n",
    "                            distance.append([idx1-idx2,chrom])\n",
    "                            coordinates_hr.append([chr, idx1, idx2])\n",
    "                            coordinates_lr.append([chr, idx1, idx2])\n",
    "                            \n",
    "                            hr_contact = hr_contacts_dict[chrom][idx1:idx1+size,idx2:idx2+size]\n",
    "                            lr_contact = lr_contacts_dict[chrom][idx1:idx1+size,idx2:idx2+size]\n",
    "\n",
    "                            crop_mats_hr.append(hr_contact)                \n",
    "                            crop_mats_lr.append(lr_contact)\n",
    "\n",
    "            crop_mats_hr = np.concatenate([item[np.newaxis,:] for item in crop_mats_hr],axis=0)\n",
    "            crop_mats_lr = np.concatenate([item[np.newaxis,:] for item in crop_mats_lr],axis=0)                \n",
    "    else:\n",
    "        assert size == 28\n",
    "        for idx1 in range(0,row-40,size):\n",
    "            for idx2 in range(0,col-40,size):\n",
    "                if abs(idx1-idx2)<thred:\n",
    "                    if quality_control(lr_contacts_dict[chrom][idx1:idx1+size,idx2:idx2+size]):\n",
    "                        distance.append([idx1-idx2,chrom])\n",
    "                        coordinates_hr.append([chr, idx1+6, idx2+6])\n",
    "                        coordinates_lr.append([chr, idx1, idx2])\n",
    "                        \n",
    "                        hr_contact = hr_contacts_dict[chrom][idx1+6:idx1+34,idx2+6:idx2+34]\n",
    "                        lr_contact = lr_contacts_dict[chrom][idx1:idx1+40,idx2:idx2+40]\n",
    "\n",
    "                        crop_mats_hr.append(hr_contact)                \n",
    "                        crop_mats_lr.append(lr_contact)\n",
    "                      \n",
    "        crop_mats_hr = np.concatenate([item[np.newaxis,:] for item in crop_mats_hr],axis=0)\n",
    "        crop_mats_lr = np.concatenate([item[np.newaxis,:] for item in crop_mats_lr],axis=0)\n",
    "        \n",
    "    return crop_mats_hr,crop_mats_lr,distance,coordinates_hr,coordinates_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델별 submatrix 생성\n",
    "def DeepHic_data_split(chrom_list):\n",
    "    random.seed(100)\n",
    "    distance_all=[]\n",
    "    assert len(chrom_list)>0\n",
    "    hr_mats,lr_mats,hr_coordinates=[],[],[]\n",
    "    for chrom in chrom_list:\n",
    "        crop_mats_hr,crop_mats_lr,distance,coordinates_hr,_ = crop_hic_matrix_by_chrom(chrom,size=submat_size,thred=200)\n",
    "        distance_all+=distance\n",
    "        hr_mats.append(crop_mats_hr)\n",
    "        lr_mats.append(crop_mats_lr)\n",
    "        hr_coordinates.append(coordinates_hr)\n",
    "    hr_mats = np.concatenate(hr_mats,axis=0)\n",
    "    lr_mats = np.concatenate(lr_mats,axis=0)\n",
    "    hr_mats=hr_mats[:,np.newaxis]\n",
    "    lr_mats=lr_mats[:,np.newaxis]\n",
    "    hr_coordinates = sum(hr_coordinates, [])\n",
    "    return hr_mats,lr_mats,hr_coordinates\n",
    "\n",
    "def HiCARN_data_split(chrom_list):\n",
    "    random.seed(100)\n",
    "    distance_all=[]\n",
    "    assert len(chrom_list)>0\n",
    "    hr_mats,lr_mats,hr_coordinates=[],[],[]\n",
    "    for chrom in chrom_list:\n",
    "        crop_mats_hr,crop_mats_lr,distance,coordinates_hr,_ = crop_hic_matrix_by_chrom(chrom,size=submat_size,thred=200)\n",
    "        distance_all+=distance\n",
    "        hr_mats.append(crop_mats_hr)\n",
    "        lr_mats.append(crop_mats_lr)\n",
    "        hr_coordinates.append(coordinates_hr)\n",
    "    hr_mats = np.concatenate(hr_mats,axis=0)\n",
    "    lr_mats = np.concatenate(lr_mats,axis=0)\n",
    "    hr_mats=hr_mats[:,np.newaxis]\n",
    "    lr_mats=lr_mats[:,np.newaxis]\n",
    "    hr_coordinates = sum(hr_coordinates, [])\n",
    "    return hr_mats,lr_mats,hr_coordinates\n",
    "\n",
    "def DFHiC_data_split(chrom_list):\n",
    "    random.seed(100)\n",
    "    distance_all=[]\n",
    "    assert len(chrom_list)>0\n",
    "    hr_mats,lr_mats=[],[]\n",
    "    for chrom in chrom_list:\n",
    "        crop_mats_hr,crop_mats_lr,distance,_,_ = crop_hic_matrix_by_chrom(chrom,size=submat_size,thred=200)\n",
    "        distance_all+=distance\n",
    "        hr_mats.append(crop_mats_hr)\n",
    "        lr_mats.append(crop_mats_lr)\n",
    "    hr_mats = np.concatenate(hr_mats,axis=0)\n",
    "    lr_mats = np.concatenate(lr_mats,axis=0)\n",
    "    hr_mats=hr_mats[:,np.newaxis]\n",
    "    lr_mats=lr_mats[:,np.newaxis]\n",
    "    hr_mats=hr_mats.transpose((0,2,3,1))\n",
    "    lr_mats=lr_mats.transpose((0,2,3,1))\n",
    "    return hr_mats,lr_mats,distance_all\n",
    "\n",
    "def HiCNN_data_split(chrom_list):\n",
    "    random.seed(100)\n",
    "    distance_all=[]\n",
    "    assert len(chrom_list)>0\n",
    "    hr_mats,lr_mats,hr_coordinates,lr_coordinates=[],[],[],[]\n",
    "    for chrom in chrom_list:\n",
    "        crop_mats_hr,crop_mats_lr,distance,coordinates_hr,coordinates_lr = crop_hic_matrix_by_chrom(chrom,size=submat_size,thred=200,model='.')\n",
    "        distance_all+=distance\n",
    "        hr_mats.append(crop_mats_hr)\n",
    "        lr_mats.append(crop_mats_lr)\n",
    "        hr_coordinates.append(coordinates_hr)\n",
    "        lr_coordinates.append(coordinates_lr)\n",
    "    hr_mats = np.concatenate(hr_mats,axis=0)\n",
    "    lr_mats = np.concatenate(lr_mats,axis=0)\n",
    "    hr_mats=hr_mats[:,np.newaxis]\n",
    "    lr_mats=lr_mats[:,np.newaxis]\n",
    "    hr_coordinates = sum(hr_coordinates, [])\n",
    "    lr_coordinates = sum(lr_coordinates, [])\n",
    "    return hr_mats,lr_mats,hr_coordinates,lr_coordinates\n",
    "\n",
    "def SRHiC_data_split(chrom_list):\n",
    "    random.seed(100)\n",
    "    distance_all=[]\n",
    "    assert len(chrom_list)>0\n",
    "    hr_mats,lr_mats=[],[]\n",
    "    for chrom in chrom_list:\n",
    "        crop_mats_hr,crop_mats_lr,_,_,_ = crop_hic_matrix_by_chrom(chrom,size=submat_size,thred=200)\n",
    "        hr_mats.append(crop_mats_hr)\n",
    "        lr_mats.append(crop_mats_lr)\n",
    "    hr_mats = np.concatenate(hr_mats,axis=0)\n",
    "    lr_mats = np.concatenate(lr_mats,axis=0)\n",
    "    hr_mats=hr_mats[:,np.newaxis]\n",
    "    lr_mats=lr_mats[:,np.newaxis]\n",
    "    return hr_mats,lr_mats\n",
    "\n",
    "def hicplus_data_split(chrom_list):\n",
    "    random.seed(100)\n",
    "    distance_all=[]\n",
    "    assert len(chrom_list)>0\n",
    "    hr_mats,lr_mats,hr_coordinates,lr_coordinates=[],[],[],[]\n",
    "    for chrom in chrom_list:\n",
    "        crop_mats_hr,crop_mats_lr,distance,coordinates_hr,coordinates_lr = crop_hic_matrix_by_chrom(chrom,size=submat_size,thred=200,model='model')\n",
    "        distance_all+=distance\n",
    "        hr_mats.append(crop_mats_hr)\n",
    "        lr_mats.append(crop_mats_lr)\n",
    "        hr_coordinates.append(coordinates_hr)\n",
    "        lr_coordinates.append(coordinates_lr)\n",
    "    hr_mats = np.concatenate(hr_mats,axis=0)\n",
    "    lr_mats = np.concatenate(lr_mats,axis=0)\n",
    "    hr_mats=hr_mats[:,np.newaxis]\n",
    "    lr_mats=lr_mats[:,np.newaxis]\n",
    "    hr_coordinates = sum(hr_coordinates, [])\n",
    "    lr_coordinates = sum(lr_coordinates, [])\n",
    "    return hr_mats,lr_mats,hr_coordinates,lr_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_data_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_314224/1788133704.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhr_contacts_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr_contacts_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_hr_contacts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_lr_contacts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhic_matrix_extraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"DFHiC\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 모델이 원하는 포멧으로 저장\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"DFHiC\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_314224/3693424233.py\u001b[0m in \u001b[0;36mhic_matrix_extraction\u001b[0;34m(res, norm_method)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mhr_contacts_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchrom_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mhr_hic_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{input_data_dir}/chr{each}_10kb.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mchrom_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{ref_chrom}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;31m# GM12878 Hg19\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mmat_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchrom_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'chr{each}'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_data_dir' is not defined"
     ]
    }
   ],
   "source": [
    "hr_contacts_dict,lr_contacts_dict,nb_hr_contacts,nb_lr_contacts = hic_matrix_extraction()\n",
    "\n",
    "# model == \"DFHiC\"\n",
    "# 모델이 원하는 포멧으로 저장\n",
    "if model == \"DFHiC\":\n",
    "    save_dir = './data_DFHiC/'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "    submat_size = 40 # 40(DFHiC, deepHiC, HiCARN) or 28(HiCNN, SRHiC)\n",
    "\n",
    "    hr_mats_train,lr_mats_train,distance_train = DFHiC_data_split([f'chr{idx}' for idx in list(range(1,18))]) # train: 1~17\n",
    "    hr_mats_test,lr_mats_test,distance_test = DFHiC_data_split([f'chr{idx}' for idx in list(range(18,23))]) # test: 18~22\n",
    "\n",
    "    np.savez(save_dir+f'train_data_raw_ratio{data_ratio}.npz', train_lr=lr_mats_train,train_hr=hr_mats_train,distance=distance_train)\n",
    "    np.savez(save_dir+f'test_data_raw_ratio{data_ratio}.npz', test_lr=lr_mats_test,test_hr=hr_mats_test,distance=distance_test)\n",
    "\n",
    "elif model == \"deepHiC\":\n",
    "    save_dir = './data_DeepHiC/'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    submat_size = 40 # 40(DFHiC, deepHiC, HiCARN) or 28(HiCNN, SRHiC)\n",
    "\n",
    "    hr_mats_train,lr_mats_train,coordinates_train = DeepHic_data_split([f'chr{idx}' for idx in list(range(1,15))]) # train:1~15\n",
    "    hr_mats_valid,lr_mats_valid,coordinates_valid = DeepHic_data_split([f'chr{idx}' for idx in list(range(15,18))]) # valid:15~17\n",
    "    hr_mats_test,lr_mats_test,coordinates_test = DeepHic_data_split([f'chr{idx}' for idx in list(range(18,23))]) # test:18~22\n",
    "\n",
    "    compacts = {int(k.split('chr')[1]) : np.nonzero(v)[0] for k, v in hr_contacts_dict.items()}\n",
    "    size = {item.split()[0].split('chr')[1]:int(item.strip().split()[1])for item in open('chromosome.txt').readlines()}\n",
    "\n",
    "    os.mkdir(save_dir+'Train_and_Validation/')\n",
    "    os.mkdir(save_dir+'Test/')\n",
    "\n",
    "    np.savez(save_dir+f'Train_and_Validation/train_ratio{data_ratio}.npz', data=lr_mats_train,target=hr_mats_train,inds=np.array(coordinates_train, dtype=np.int_),compacts=compacts,size=size)\n",
    "    np.savez(save_dir+f'Train_and_Validation/valid_ratio{data_ratio}.npz', data=lr_mats_valid,target=hr_mats_valid,inds=np.array(coordinates_valid, dtype=np.int_),compacts=compacts,size=size)\n",
    "    np.savez(save_dir+f'Test/test_ratio{data_ratio}.npz', data=lr_mats_test,target=hr_mats_test,inds=np.array(coordinates_test, dtype=np.int_),compacts=compacts,size=size)\n",
    "    \n",
    "elif model == \"HiCARN\":\n",
    "    save_dir = './data_HiCARN/'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "    submat_size = 40 # 40(DFHiC, deepHiC, HiCARN) or 28(HiCNN, SRHiC)\n",
    "\n",
    "    hr_mats_train,lr_mats_train,coordinates_train = HiCARN_data_split([f'chr{idx}' for idx in list(range(1,15))]) # train:1~14\n",
    "    hr_mats_valid,lr_mats_valid,coordinates_valid = HiCARN_data_split([f'chr{idx}' for idx in list(range(15,18))]) # valid:15~17\n",
    "    hr_mats_test,lr_mats_test,coordinates_test = HiCARN_data_split([f'chr{idx}' for idx in list(range(18,23))]) # test:18~22\n",
    "\n",
    "    compacts = {int(k.split('chr')[1]) : np.nonzero(v)[0] for k, v in hr_contacts_dict.items()}\n",
    "    size = {item.split()[0].split('chr')[1]:int(item.strip().split()[1])for item in open('chromosome.txt').readlines()}\n",
    "\n",
    "    os.mkdir(save_dir+'Train_and_Validation/')\n",
    "    os.mkdir(save_dir+'Test/')\n",
    "\n",
    "    np.savez(save_dir+f'Train_and_Validation/train_ratio{data_ratio}.npz', data=lr_mats_train,target=hr_mats_train,inds=np.array(coordinates_train, dtype=np.int_),compacts=compacts,size=size)\n",
    "    np.savez(save_dir+f'Train_and_Validation/valid_ratio{data_ratio}.npz', data=lr_mats_valid,target=hr_mats_valid,inds=np.array(coordinates_valid, dtype=np.int_),compacts=compacts,size=size)\n",
    "    np.savez(save_dir+f'Test/test_ratio{data_ratio}.npz', data=lr_mats_test,target=hr_mats_test,inds=np.array(coordinates_test, dtype=np.int_),compacts=compacts,size=size)\n",
    "\n",
    "elif model == \"HiCNN\":\n",
    "    save_dir = './data_HiCNN/'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "    submat_size = 40 # 40(DFHiC, deepHiC, HiCARN) or 28(HiCNN, SRHiC)\n",
    "\n",
    "    hr_mats_train,lr_mats_train,hr_coordinates_train,lr_coordinates_train = HiCNN_data_split([f'chr{idx}' for idx in list(range(1,15))]) # train:1~14\n",
    "    hr_mats_valid,lr_mats_valid,hr_coordinates_valid,lr_coordinates_valid = HiCNN_data_split([f'chr{idx}' for idx in list(range(15,18))]) # valid:15~17\n",
    "    # hr_mats_test,lr_mats_test,hr_coordinates_test,lr_coordinates_test = HiCNN_data_split([f'chr{idx}' for idx in list(range(18,23))]) # test:18~22\n",
    "\n",
    "    np.save(save_dir+f'subMats_train_target_ratio{data_ratio}', hr_mats_train)\n",
    "    np.save(save_dir+f'subMats_train_ratio{data_ratio}', lr_mats_train)\n",
    "    np.save(save_dir+f'index_train_target', hr_coordinates_train)\n",
    "    np.save(save_dir+f'index_train_data', lr_coordinates_train)\n",
    "    np.save(save_dir+f'subMats_valid_target_ratio{data_ratio}', hr_mats_valid)\n",
    "    np.save(save_dir+f'subMats_valid_ratio{data_ratio}', lr_mats_valid)\n",
    "    np.save(save_dir+f'index_valid_target', hr_coordinates_valid)\n",
    "    np.save(save_dir+f'index_valid_data', lr_coordinates_valid)\n",
    "    \n",
    "elif model == \"SRHiC\":\n",
    "    save_dir = './data_SRHiC/'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "    submat_size = 28 # 40(DFHiC, deepHiC, HiCARN) or 28(HiCNN, SRHiC)\n",
    "\n",
    "    hr_mats_train,lr_mats_train = SRHiC_data_split([f'chr{idx}' for idx in list(range(1,18))]) # train: 1~17\n",
    "    hr_mats_test,lr_mats_test = SRHiC_data_split([f'chr{idx}' for idx in list(range(18,23))]) # valid:15~17\n",
    "\n",
    "    train = np.concatenate((lr_mats_train[:,0,:,:], np.concatenate((hr_mats_train[:,0,:,:],np.zeros((hr_mats_train.shape[0],12,28))), axis=1)), axis=2)\n",
    "    valid = np.concatenate((lr_mats_test[:,0,:,:], np.concatenate((hr_mats_test[:,0,:,:],np.zeros((hr_mats_test.shape[0],12,28))), axis=1)), axis=2)\n",
    "\n",
    "    np.save(save_dir+f'train_data_raw_ratio{data_ratio}', train)\n",
    "    np.save(save_dir+f'valid_data_raw_ratio{data_ratio}', valid)\n",
    "    \n",
    "else:\n",
    "    assert model == \"hicplus\"\n",
    "    save_dir = './data_hicplus/'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "    submat_size = 40 # 40(DFHiC, deepHiC, HiCARN, hicplus) or 28(HiCNN, SRHiC)\n",
    "\n",
    "    hr_mats_train,lr_mats_train,hr_coordinates_train,lr_coordinates_train = hicplus_data_split([f'chr{idx}' for idx in list(range(1,18))]) # train:1~17\n",
    "    hr_mats_test,lr_mats_test,hr_coordinates_test,lr_coordinates_test = hicplus_data_split([f'chr{idx}' for idx in list(range(18,23))]) # test:18~22\n",
    "\n",
    "    np.save(save_dir+f'subMats_train_target_ratio{data_ratio}', hr_mats_train)\n",
    "    np.save(save_dir+f'subMats_train_ratio{data_ratio}', lr_mats_train)\n",
    "    np.save(save_dir+f'index_train_target', hr_coordinates_train)\n",
    "    np.save(save_dir+f'index_train_data', lr_coordinates_train)\n",
    "    np.save(save_dir+f'subMats_test_target_ratio{data_ratio}', hr_mats_test)\n",
    "    np.save(save_dir+f'subMats_test_ratio{data_ratio}', lr_mats_test)\n",
    "    np.save(save_dir+f'index_test_target', hr_coordinates_test)\n",
    "    np.save(save_dir+f'index_test_data', lr_coordinates_test)  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffuser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
