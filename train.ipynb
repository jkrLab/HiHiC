{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, math, random, argparse\n",
    "import numpy as np\n",
    "\n",
    "path = os.getcwd()\n",
    "random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인자 받기\n",
    "parser = argparse.ArgumentParser(description='Train deep learning model for enhancing Hi-C contact map', add_help=True)\n",
    "req_args = parser.add_argument_group('Required Arguments')\n",
    "req_args.add_argument('-i', dest='input_data_dir', required=True,\n",
    "                      help='REQUIRED: Hi-C data directory containig numpy matrix files (Directory of Hi-C matrix for training input) - (example) /HiHiC-main/data_DFHiC')\n",
    "req_args.add_argument('-o', dest='output_model_dir', required=True,\n",
    "                      help='REQUIRED: Directory to save training weight (Directory for saving pretrained model) - (example) /HiHiC-main/pretrained')\n",
    "req_args.add_argument('-m', dest='model', required=True, choices=['HiCARN', 'DeepHiC', 'HiCNN2', 'HiCSR', 'DFHiC', 'hicplus', 'SRHiC'],\n",
    "                      help='REQUIRED: Model name that you want to use (One of HiCARN, DeepHiC, HiCNN2, HiCSR, DFHiC, hicplus, and SRHiC) - (example) DFHiC')\n",
    "req_args.add_argument('-e', dest='epoch', required=True,\n",
    "                      help='REQUIRED: Number of train epoch - (example) 500')\n",
    "req_args.add_argument('-g', dest='gpu', required=True,\n",
    "                      help='REQUIRED: Number of gpu for training - (example) 0')\n",
    "req_args.add_argument('-p', dest='output_performance_dir', required=True,\n",
    "                      help='REQUIRED: Directory to save training performance trend - (example) HiHiC-main/performance')\n",
    "\n",
    "args = parser.parse_args()\n",
    "input_data_dir = args.input_data_dir \n",
    "output_model_dir = args.output_model_dir\n",
    "model = args.model\n",
    "epoch = args.epoch\n",
    "gpu = args.gpu\n",
    "output_performance_dir = args.output_performance_dir\n",
    "print(f\"\\n\\n\\n...Start {model} training...\\nusing Hi-C data of {input_data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = f'{output_dir}/data_{model}/'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)   \n",
    "\n",
    "# 모델이 원하는 포멧으로 저장\n",
    "if model == \"DFHiC\":\n",
    "    hr_mats_train,lr_mats_train,distance_train = DFHiC_data_split([f'chr{idx}' for idx in list(range(1,18))]) # train: 1~17\n",
    "    hr_mats_test,lr_mats_test,distance_test = DFHiC_data_split([f'chr{idx}' for idx in list(range(18,23))]) # test: 18~22\n",
    "\n",
    "    np.savez(save_dir+f'train_data_raw_ratio{data_ratio}.npz', train_lr=lr_mats_train,train_hr=hr_mats_train,distance=distance_train)\n",
    "    np.savez(save_dir+f'test_data_raw_ratio{data_ratio}.npz', test_lr=lr_mats_test,test_hr=hr_mats_test,distance=distance_test)\n",
    "\n",
    "elif model == \"deepHiC\":      \n",
    "    hr_mats_train,lr_mats_train,coordinates_train = DeepHiC_data_split([f'chr{idx}' for idx in list(range(1,15))]) # train:1~15\n",
    "    hr_mats_valid,lr_mats_valid,coordinates_valid = DeepHiC_data_split([f'chr{idx}' for idx in list(range(15,18))]) # valid:15~17\n",
    "    hr_mats_test,lr_mats_test,coordinates_test = DeepHiC_data_split([f'chr{idx}' for idx in list(range(18,23))]) # test:18~22\n",
    "\n",
    "    compacts = {int(k.split('chr')[1]) : np.nonzero(v)[0] for k, v in hr_contacts_dict.items()}\n",
    "    size = {item.split()[0].split('chr')[1]:int(item.strip().split()[1])for item in open('chromosome.txt').readlines()}\n",
    "\n",
    "    os.mkdir(save_dir+'Train_and_Validation/')\n",
    "    os.mkdir(save_dir+'Test/')\n",
    "\n",
    "    np.savez(save_dir+f'Train_and_Validation/train_ratio{data_ratio}.npz', data=lr_mats_train,target=hr_mats_train,inds=np.array(coordinates_train, dtype=np.int_),compacts=compacts,size=size)\n",
    "    np.savez(save_dir+f'Train_and_Validation/valid_ratio{data_ratio}.npz', data=lr_mats_valid,target=hr_mats_valid,inds=np.array(coordinates_valid, dtype=np.int_),compacts=compacts,size=size)\n",
    "    np.savez(save_dir+f'Test/test_ratio{data_ratio}.npz', data=lr_mats_test,target=hr_mats_test,inds=np.array(coordinates_test, dtype=np.int_),compacts=compacts,size=size)\n",
    "    \n",
    "elif model == \"HiCARN\":          \n",
    "    hr_mats_train,lr_mats_train,coordinates_train = HiCARN_data_split([f'chr{idx}' for idx in list(range(1,15))]) # train:1~14\n",
    "    hr_mats_valid,lr_mats_valid,coordinates_valid = HiCARN_data_split([f'chr{idx}' for idx in list(range(15,18))]) # valid:15~17\n",
    "    hr_mats_test,lr_mats_test,coordinates_test = HiCARN_data_split([f'chr{idx}' for idx in list(range(18,23))]) # test:18~22\n",
    "\n",
    "    compacts = {int(k.split('chr')[1]) : np.nonzero(v)[0] for k, v in hr_contacts_dict.items()}\n",
    "    size = {item.split()[0].split('chr')[1]:int(item.strip().split()[1])for item in open('chromosome.txt').readlines()}\n",
    "\n",
    "    os.mkdir(save_dir+'Train_and_Validation/')\n",
    "    os.mkdir(save_dir+'Test/')\n",
    "\n",
    "    np.savez(save_dir+f'Train_and_Validation/train_ratio{data_ratio}.npz', data=lr_mats_train,target=hr_mats_train,inds=np.array(coordinates_train, dtype=np.int_),compacts=compacts,size=size)\n",
    "    np.savez(save_dir+f'Train_and_Validation/valid_ratio{data_ratio}.npz', data=lr_mats_valid,target=hr_mats_valid,inds=np.array(coordinates_valid, dtype=np.int_),compacts=compacts,size=size)\n",
    "    np.savez(save_dir+f'Test/test_ratio{data_ratio}.npz', data=lr_mats_test,target=hr_mats_test,inds=np.array(coordinates_test, dtype=np.int_),compacts=compacts,size=size)\n",
    "\n",
    "elif model == \"HiCNN\":     \n",
    "    hr_mats_train,lr_mats_train,hr_coordinates_train,lr_coordinates_train = HiCNN_data_split([f'chr{idx}' for idx in list(range(1,15))]) # train:1~14\n",
    "    hr_mats_valid,lr_mats_valid,hr_coordinates_valid,lr_coordinates_valid = HiCNN_data_split([f'chr{idx}' for idx in list(range(15,18))]) # valid:15~17\n",
    "    # hr_mats_test,lr_mats_test,hr_coordinates_test,lr_coordinates_test = HiCNN_data_split([f'chr{idx}' for idx in list(range(18,23))]) # test:18~22\n",
    "\n",
    "    np.save(save_dir+f'subMats_train_target_ratio{data_ratio}', hr_mats_train)\n",
    "    np.save(save_dir+f'subMats_train_ratio{data_ratio}', lr_mats_train)\n",
    "    np.save(save_dir+f'index_train_target', hr_coordinates_train)\n",
    "    np.save(save_dir+f'index_train_data', lr_coordinates_train)\n",
    "    np.save(save_dir+f'subMats_valid_target_ratio{data_ratio}', hr_mats_valid)\n",
    "    np.save(save_dir+f'subMats_valid_ratio{data_ratio}', lr_mats_valid)\n",
    "    np.save(save_dir+f'index_valid_target', hr_coordinates_valid)\n",
    "    np.save(save_dir+f'index_valid_data', lr_coordinates_valid)\n",
    "    \n",
    "elif model == \"SRHiC\":  \n",
    "    hr_mats_train,lr_mats_train = SRHiC_data_split([f'chr{idx}' for idx in list(range(1,18))]) # train: 1~17\n",
    "    hr_mats_test,lr_mats_test = SRHiC_data_split([f'chr{idx}' for idx in list(range(18,23))]) # valid:15~17\n",
    "\n",
    "    train = np.concatenate((lr_mats_train[:,0,:,:], np.concatenate((hr_mats_train[:,0,:,:],np.zeros((hr_mats_train.shape[0],12,28))), axis=1)), axis=2)\n",
    "    valid = np.concatenate((lr_mats_test[:,0,:,:], np.concatenate((hr_mats_test[:,0,:,:],np.zeros((hr_mats_test.shape[0],12,28))), axis=1)), axis=2)\n",
    "\n",
    "    np.save(save_dir+f'train_data_raw_ratio{data_ratio}', train)\n",
    "    np.save(save_dir+f'valid_data_raw_ratio{data_ratio}', valid)\n",
    "    \n",
    "else:\n",
    "    assert model == \"hicplus\"\n",
    "    hr_mats_train,lr_mats_train,hr_coordinates_train,lr_coordinates_train = hicplus_data_split([f'chr{idx}' for idx in list(range(1,18))]) # train:1~17\n",
    "    hr_mats_test,lr_mats_test,hr_coordinates_test,lr_coordinates_test = hicplus_data_split([f'chr{idx}' for idx in list(range(18,23))]) # test:18~22\n",
    "\n",
    "    np.save(save_dir+f'subMats_train_target_ratio{data_ratio}', hr_mats_train)\n",
    "    np.save(save_dir+f'subMats_train_ratio{data_ratio}', lr_mats_train)\n",
    "    np.save(save_dir+f'index_train_target', hr_coordinates_train)\n",
    "    np.save(save_dir+f'index_train_data', lr_coordinates_train)\n",
    "    np.save(save_dir+f'subMats_test_target_ratio{data_ratio}', hr_mats_test)\n",
    "    np.save(save_dir+f'subMats_test_ratio{data_ratio}', lr_mats_test)\n",
    "    np.save(save_dir+f'index_test_target', hr_coordinates_test)\n",
    "    np.save(save_dir+f'index_test_data', lr_coordinates_test)  \n",
    "    \n",
    "print(f\"\\n\\n... Generated data is saved in {save_dir}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight 저장\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "# weight 불러오기\n",
    "model = TheModelClass(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"training_1/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "# Train the model with the new callback\n",
    "model.fit(train_images, \n",
    "          train_labels,  \n",
    "          epochs=10,\n",
    "          validation_data=(test_images, test_labels),\n",
    "          callbacks=[cp_callback])  # Pass callback to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "sec = time.time()-start\n",
    "times = str(datetime.timedelta(seconds=sec))\n",
    "short = times.split(\".\")[0] # 초 단위 까지만\n",
    "print(f\"{short} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE = np.square(np.subtract(x, y)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signaltonoise(Arr, axis=0, ddof=0):\n",
    "    Arr = np.asanyarray(Arr)\n",
    "    me = Arr.mean(axis)\n",
    "    sd = Arr.std(axis=axis, ddof=ddof)\n",
    "    return np.where(sd == 0, 0, me/sd)\n",
    "\n",
    "Arr=[[20, 4, 7, 1, 34], [50, 12, 15, 34, 5]]\n",
    "print(signaltonoise(Arr,axis=0,ddof=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## version1\n",
    "def PSNR(original, compressed): \n",
    "    mse = np.mean((original - compressed) ** 2) \n",
    "    if(mse == 0):  # MSE is zero means no noise is present in the signal . \n",
    "                  # Therefore PSNR have no importance. \n",
    "        return 100\n",
    "    max_pixel = 255.0\n",
    "    psnr = 20 * log10(max_pixel / sqrt(mse)) \n",
    "    return psnr \n",
    "  \n",
    "def main(): \n",
    "     original = cv2.imread(\"original_image.png\") \n",
    "     compressed = cv2.imread(\"compressed_image.png\", 1) \n",
    "     value = PSNR(original, compressed) \n",
    "     print(f\"PSNR value is {value} dB\") \n",
    "     \n",
    "     \n",
    "## version2     \n",
    "def calculate_psnr(img1, img2):\n",
    "    # img1 and img2 have range [0, 255]\n",
    "    img1 = img1.astype(np.float64)\n",
    "    img2 = img2.astype(np.float64)\n",
    "    mse = np.mean((img1 - img2)**2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    return 20 * math.log10(255.0 / math.sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def ssim(img1, img2):\n",
    "    C1 = (0.01 * 255)**2\n",
    "    C2 = (0.03 * 255)**2\n",
    "\n",
    "    img1 = img1.astype(np.float64)\n",
    "    img2 = img2.astype(np.float64)\n",
    "    kernel = cv2.getGaussianKernel(11, 1.5)\n",
    "    window = np.outer(kernel, kernel.transpose())\n",
    "\n",
    "    mu1 = cv2.filter2D(img1, -1, window)[5:-5, 5:-5]  # valid\n",
    "    mu2 = cv2.filter2D(img2, -1, window)[5:-5, 5:-5]\n",
    "    mu1_sq = mu1**2\n",
    "    mu2_sq = mu2**2\n",
    "    mu1_mu2 = mu1 * mu2\n",
    "    sigma1_sq = cv2.filter2D(img1**2, -1, window)[5:-5, 5:-5] - mu1_sq\n",
    "    sigma2_sq = cv2.filter2D(img2**2, -1, window)[5:-5, 5:-5] - mu2_sq\n",
    "    sigma12 = cv2.filter2D(img1 * img2, -1, window)[5:-5, 5:-5] - mu1_mu2\n",
    "\n",
    "    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) *\n",
    "                                                            (sigma1_sq + sigma2_sq + C2))\n",
    "    return ssim_map.mean()\n",
    "\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    '''calculate SSIM\n",
    "    the same outputs as MATLAB's\n",
    "    img1, img2: [0, 255]\n",
    "    '''\n",
    "    if not img1.shape == img2.shape:\n",
    "        raise ValueError('Input images must have the same dimensions.')\n",
    "    if img1.ndim == 2:\n",
    "        return ssim(img1, img2)\n",
    "    elif img1.ndim == 3:\n",
    "        if img1.shape[2] == 3:\n",
    "            ssims = []\n",
    "            for i in range(3):\n",
    "                ssims.append(ssim(img1, img2))\n",
    "            return np.array(ssims).mean()\n",
    "        elif img1.shape[2] == 1:\n",
    "            return ssim(np.squeeze(img1), np.squeeze(img2))\n",
    "    else:\n",
    "        raise ValueError('Wrong input image dimensions.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
